name: ETL Data Cleaning Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'backend/app/db/**'
      - 'backend/app/etl/**'
      - '.github/workflows/etl-pipeline.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'backend/app/db/**'
      - 'backend/app/etl/**'
  workflow_dispatch:  # Allow manual trigger

jobs:
  data-cleaning:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        cd backend
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install psutil  # For memory monitoring
        
    - name: Create backup directory
      run: |
        cd backend
        mkdir -p backups
        
    - name: Run data cleaning pipeline
      run: |
        cd backend
        python -m app.etl.data_cleaner
      id: data-cleaning
      
    - name: Run query optimization tests
      run: |
        cd backend
        python -m app.etl.query_optimizer
      id: query-optimization
      
    - name: Generate ETL report
      run: |
        cd backend
        echo "ETL Pipeline completed successfully"
        echo "Backup created in backups/ directory"
        echo "Query optimization tests completed"
        
    - name: Upload ETL artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: etl-artifacts
        path: |
          backend/backups/
          backend/app/db/*.json
        retention-days: 30
        
    - name: Comment on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const { data: comments } = await github.rest.issues.listComments({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
          });
          
          const botComment = comments.find(comment => 
            comment.user.type === 'Bot' && 
            comment.body.includes('ETL Pipeline Results')
          );
          
          const commentBody = `## üîß ETL Pipeline Results
          
          ‚úÖ **Data Cleaning**: Completed successfully
          ‚úÖ **Query Optimization**: Performance tests completed
          ‚úÖ **Backup Created**: Data backed up before cleaning
          
          ### üìä Summary
          - Data quality issues identified and fixed
          - Query performance optimized
          - Backup artifacts uploaded
          
          ### üìÅ Artifacts
          - Backup files: Available in artifacts
          - Cleaned data: Updated in database files
          - Performance reports: Generated and logged
          
          ---
          *This comment was automatically generated by the ETL pipeline*`;
          
          if (botComment) {
            await github.rest.issues.updateComment({
              comment_id: botComment.id,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: commentBody,
            });
          } else {
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: commentBody,
            });
          }

  performance-monitoring:
    runs-on: ubuntu-latest
    needs: data-cleaning
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        cd backend
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install psutil
        
    - name: Run performance benchmarks
      run: |
        cd backend
        echo "Running performance benchmarks..."
        python -c "
        from app.etl.query_optimizer import QueryOptimizer
        from app.db.database import db
        
        optimizer = QueryOptimizer()
        
        # Run multiple iterations for better accuracy
        for i in range(5):
            result = optimizer.explain_analyze('get_tasks_due_this_week', db.get_tasks_due_this_week)
            print(f'Iteration {i+1}: {result[\"metrics\"][\"execution_time_ms\"]:.2f}ms')
        
        report = optimizer.generate_performance_report()
        print(f'Average execution time: {report[\"performance\"][\"average_execution_time_ms\"]:.2f}ms')
        "
        
    - name: Upload performance report
      uses: actions/upload-artifact@v4
      with:
        name: performance-report
        path: backend/performance_report.json
        retention-days: 30

  security-scan:
    runs-on: ubuntu-latest
    needs: data-cleaning
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Run Bandit security scan
      run: |
        cd backend
        pip install bandit
        bandit -r app/ -f json -o bandit-report.json || true
        
    - name: Upload security report
      uses: actions/upload-artifact@v4
      with:
        name: security-report
        path: backend/bandit-report.json
        retention-days: 30 